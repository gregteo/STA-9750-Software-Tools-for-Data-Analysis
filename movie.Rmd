---
title: "Models on predicting movie rating"
author: "C.Y., M.S., G.T., P.C."
output: 
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, dev = "png", dpi=300) 
library(tidyverse)
library(jsonlite)
library(lubridate)
library(modelr)
library(randomForest)
library(rpart)
library(cowplot)
comma <- function(x) format(x, digits = 3, big.mark = ",")
```

## STA 9750 Final Project

### Introduction

This project aims to predict movies' ratings released  on or before July 2017 by using various variables including budget, adult, runtime, release date, genre and production company. At the end of the report, we also explore the relationships between rating and revenue or popularity. Our data was downloaded from [kaggle](https://www.kaggle.com/rounakbanik/the-movies-dataset?select=movies_metadata.csv).

```{r combine and clean data, warning=FALSE, include=FALSE}
metadata <- read_csv("movies_metadata.csv")

tibble_rating <- read_csv("ratings.csv") 
rating <- tibble_rating %>% group_by(movieId) %>% summarise(m_rating = median(rating))

link <- read_csv("links.csv")  %>% inner_join(rating) %>% rename(id = tmdbId)

tibble_all <- link %>% inner_join(metadata) %>% 
  select(m_rating, id, budget, genres, production_companies, adult, runtime, original_language,
         release_date, revenue, popularity) %>% 
  mutate(lrating = log2(m_rating)) 

all <- tibble_all %>% filter(original_language == "en") 

datetime <- all %>% pull(release_date) %>% mdy()
releaseYear <- ifelse(as.double(year(datetime)) > 2017, (as.double(year(datetime)) - 100), as.double(year(datetime)))
all <- all %>% cbind(releaseYear)

all_genres <- NULL
for (i in all$genres){
  g <- fromJSON(gsub("'", '"', i))$name
  all_genres <- c(all_genres, g)
}
for (current_genre in unique(all_genres)) {  
  all <- all %>% mutate(!!current_genre := grepl(current_genre, all$genres))
}

all_company <- NULL
for (i in all$production_companies){
  p <- fromJSON(gsub("'", '"', i))$name
  all_company <- c(all_company, p)
}
all_company2 <- as_tibble(sort(xtabs(~all_company), decreasing = TRUE)) 
for (current_company in all_company2 %>% head(50) %>% pull(all_company)) {  
  all <- all %>% mutate(!!current_company := grepl(current_company, all$production_companies))
}

newnames <- NULL
for (cn in colnames(all)) {
  newnames <- c(newnames, gsub("[^[:alnum:]]", "", cn))
}
colnames(all) <- newnames
```

The movies dataset was collected from `r comma(nrow(metadata))` movies with `r comma(nrow(tibble_rating))` ratings. Each movie was rated by many users, so we used the median rating to represent the rating of the movie. Then we combine the rating file with the movie metadata file via the link file. Hereby, we are left with `r comma(nrow(tibble_all))` movies.

We also notice that there are some movies which are not in English. To keep the consistency, we filtered out `r comma(sum(!(tibble_all$original_language == "en"), na.rm = TRUE))` movies whose original language are not English  and `r comma(sum(is.na(tibble_all$original_language)))` movies whose original language are not available. In addition, we also filtered out `r comma(sum(is.na(all$releaseYear)))` movies which don't provide clear information about release date and `r comma(sum(is.na(all$runtime)))` movies whose runtime are missing.

```{r filter data,echo=FALSE}
all <- all %>% filter(!is.na(releaseYear)) %>% filter(!is.na(runtime))
```

Therefore, we are left with `r comma(nrow(all))` rows of data. The first ten rows and some columns of the data are showed below:

```{r display tibble,echo=FALSE}
knitr::kable(all[1:10,c(1,3,6,7,13,14,15,34,36)])
```

Next we split the data with 80% training data and 20% test data. We will fit our models on the training data and validate the models on the test data.

```{r split into train and test,include=FALSE}
train_indices <- sample(1:nrow(all), 0.8 * nrow(all))
all_train <- all %>% slice(train_indices)
all_test <- all %>% slice(-train_indices)
```

### Summary Statistics

Below is the bar chart of all ratings in the training data:

```{r rating bar chart, echo=FALSE}
all_train %>% ggplot(aes(x = mrating)) + 
  geom_bar() +
  ggtitle("Movie ratings (training data)") +
  labs(x = "Movie rating", y = "Count") 
```

From the bar chart, we can easily tell most of the movie ratings fall into the range between 3 and 4. From which, 3 has the greatest count, followed by 3.5 and 4. Looking at the table below, we can verify that the median movie rating is `r comma(median(all_train$mrating))` and the mean movie rating is `r comma(mean(all_train$mrating))`. Also, we create a table to summarize the variables containing mrating, budget, adult, runtime, and releaseYear. In addition, we make two bar charts below to show all the genres distribution and the first twenty production companies distribution.

```{r summary, echo=FALSE}
knitr::kable(all_train %>% select(mrating, budget, adult, runtime, releaseYear) %>% summary())
```

```{r genre bar chart,echo=FALSE}
as_tibble(sort(xtabs(~all_genres), decreasing = TRUE)) %>% 
  ggplot(aes(x = all_genres, y = n)) + geom_col() + coord_flip() +
  ggtitle("Genre Distribution") +
  labs(x = "Genre", y = "Count" )
```

```{r production company bar chart,echo=FALSE}
all_company2 %>% head(20) %>%
  ggplot(aes(x = all_company, y = n)) + geom_col() + coord_flip() +
  ggtitle("Top 20 Production Comapny Distribution") +
  labs(x = "Production Comapny", y = "Count")
```

### Models and Analysis

#### Random Forest Model

We begin the analysis by fitting the Random Forest Model with all the independent variables: adult, runtime, budget, releaseYear, twenty genres, as well as top fifty production companies to know the most important variables which affect predicting movie ratings mostly.

```{r random forest model 1, echo=FALSE, include=FALSE}
forest_rating1 <- randomForest(mrating ~ .,
                            data = all_train %>% 
                              select(mrating, adult, runtime, budget, releaseYear, last_col(0:69)),
                            importance = TRUE, do.trace = 10, ntree = 100)
```

The importance table is listed below. From which, we can easily tell runtime, releaseYear, budget, and genres are important variables. 

```{r importance table, echo=FALSE}

knitr::kable(head(importance(forest_rating1, type = 1)[order(-importance(forest_rating1, type=1)),]), col.names = '%IncMSE')

```

Therefore, we drop the two variables (adult and production company) and then fit another random forest model using the remaining important variables only and below is a summary and plot of the model. 

```{r random forest model 2, echo=FALSE, include=FALSE}
forest_rating2 <- randomForest(mrating ~ .,
                            data = all_train %>% 
                              select(mrating, runtime, budget, releaseYear, last_col(50:69)),
                            importance = TRUE, do.trace = 10, ntree = 100)
```

```{r summary of the random forest model 2, echo = FALSE}
forest_rating2
plot(forest_rating2)
```

#### Linear Regression Model

Now we fit Linear Regression Model by using the important variables mentioned above.

First we start to predict movie ratings by each of the important independent variables: budget, runtime, releaseYear, and genres respectively. Then we consider the linear regression model with all the important variables. Here, we use lrating, which is the log transformation of mrating, as the dependent variable.

```{r linear regression models, include=FALSE}
mod_rating_budget <- lm(lrating ~ budget, data = all_train)
mod_rating_runtime <- lm(lrating ~ runtime, data = all_train)
mod_rating_releaseYear <- lm(lrating ~ releaseYear, data= all_train)
mod_rating_genre <- lm(lrating ~ Animation+Comedy+Family+Adventure+Fantasy+Romance+Drama+Action+Crime+Thriller+Horror+History+ScienceFiction+Mystery+War+Music+Documentary+Foreign+Western+TVMovie, data = all_train)
mod_rating_all <- lm(lrating ~ ., data = all_train %>% 
                   select(lrating, budget, runtime, releaseYear, last_col(50:69)))
```

Then we make some plots to see whether the models fit the data well.

```{r linear regression model plots, echo=FALSE}

plot_budget <- all_train %>% ggplot(aes(x = budget, y = lrating)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method="lm", formula = "y ~ x")
plot_runtime <- all_train %>% ggplot(aes(x = runtime, y = lrating)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method="lm", formula = "y ~ x")
plot_releaseYear <- all_train %>% ggplot(aes(x = releaseYear, y = lrating)) +
  geom_point(alpha = 0.6) +
  geom_smooth(method="lm", formula = "y ~ x")
plot_genre <- all_train %>% 
  ggplot(aes(x = Animation+Comedy+Family+Adventure+Fantasy+Romance+Drama+Action+Crime+Thriller+Horror+History+ScienceFiction+Mystery+War+Music+Documentary+Foreign+Western+TVMovie, y = lrating)) +
  geom_point(alpha = 0.6) +
  xlab('genre') +
  geom_smooth(method="lm", formula = "y ~ x")
plot_all <- all_train %>% ggplot(aes(x = budget+runtime+releaseYear+last(0:69), y = lrating)) +
  geom_point(alpha = 0.6) +
  xlab('All') +
  geom_smooth(method="lm", formula = "y ~ x")

Plot_grid <- plot_grid(plot_budget, plot_runtime, plot_releaseYear, plot_genre, plot_all, ncol = 2, nrow = 3)
title <- ggdraw() + draw_label("Plots on Train Data", fontface = 'bold')
plot_grid(title, Plot_grid, ncol = 1, rel_heights = c(0.2, 1.2))
```

Here, we also calculate the Akaike information criterion (AIC) for the linear regression models. AIC is an estimator of out-of-sample prediction error and an estimator of the relative amount of information lost by a given model. So when comparing models fitted to the same data, the smaller the AIC, the better the model fit. The AIC for our models with budget, that with runtime, that with releaseYear, that with genres, and that with all the important variables are `r comma(AIC(mod_rating_budget))`, `r comma(AIC(mod_rating_runtime))`, `r comma(AIC(mod_rating_releaseYear))`, `r comma(AIC(mod_rating_genre))`, and `r comma(AIC(mod_rating_all))` respectively.

It is easy to find that the linear regression model with all the important variables has the lowest AIC, which means the model fits the data best among all these linear regression models. This is also fairly reasonable in reality. The model is shown below:

```{r summary of the linear regression model, echo = FALSE}
summary(mod_rating_all)
```

However, we observe that the R-squared is relatively low for this model even though it performs best among all the linear regression models. Therefore, we may draw two hypothesis here. 

##### Hypothesis

The first hypothesis is that linear regression model is not a good model for this dataset. Thus, it can be explained why the R-squared is low even with the best linear regression model. We may verify this hypothesis in the later validation on the test data part.

The second hypothesis is that the current variables are not sufficient to predict movie ratings efficiently. In order to have a better performance, we may need more variables to be involved in the model. We may confirm this hypothesis in the later conclusion part.

Now we will introduce another tree- based model to check the model performance.

#### Regression Tree Model

We decide to contain all the important variables to predict movie ratings for the regression tree model. First, we fit five trees of increasing complexity on all important variables on train data. 

```{r Regression Tree Models,echo=FALSE, include=FALSE}
tree_rating1 <- rpart(mrating ~ ., 
                          data = all_train %>%
                            select(mrating, budget, runtime, releaseYear, last_col(50:69)), control = rpart.control(cp = 0.01))

tree_rating2 <- rpart(mrating ~ ., 
                          data = all_train %>%
                            select(mrating, budget, runtime, releaseYear, last_col(50:69)), control = rpart.control(cp = 0.001))

tree_rating3 <- rpart(mrating ~ ., 
                          data = all_train %>%
                            select(mrating, budget, runtime, releaseYear, last_col(50:69)), control = rpart.control(cp = 0.0001))

tree_rating4 <- rpart(mrating ~ ., 
                          data = all_train %>%
                            select(mrating, budget, runtime, releaseYear, last_col(50:69)), control = rpart.control(cp = 0.00001))

tree_rating5 <- rpart(mrating ~ ., 
                          data = all_train %>%
                            select(mrating, budget, runtime, releaseYear, last_col(50:69)), control = rpart.control(cp = 0.000001))
```

Then we show the second tree model plot as a representation as follows. We may find Horror and Documentary in the genre category, runtime, releaseYear, budget are all important predictive variables, which verifies what we drive from the random forest model.

``` {r show and plot model, echo = FALSE}
tree_rating2
plot(tree_rating2, uniform = TRUE)
text(tree_rating2, cex = 0.7)
```

#### Validation on Test Data

We will evaluate these models by RMSE on test data respectively. 

First, we directly calculate the rmse for random forest models. They are `r comma(rmse(forest_rating1, all_test))` and `r comma(rmse(forest_rating2, all_test))`. Apparently, random forest 2 model has a lower rmse.

Next, we will use a plot to find the "elbow" for tree_based models, which means the error will improve very little even though the tree continues to grow at that point. This is a nice choice of model, which gives a trade-off between performance and complexity. 

```{r "elbow plot", echo= FALSE}
plot(c(0.01, 0.001, 0.0001, 0.00001, 0.0000001), 
     c(rmse(tree_rating1, all_test), rmse(tree_rating2, all_test), 
       rmse(tree_rating3, all_test), rmse(tree_rating4, all_test), 
       rmse(tree_rating5, all_test)),
     log = 'x', type = 'o', xlim=c(0.01, 0.0000001),
     xlab = "complexity penalty", 
     ylab = "rmse of model")
```

From the plot, we can tell that when cp equals 0.001, we get the lowest rmse on test data for tree_based models, which is exactly the second tree model which we show above with `r comma(rmse(tree_rating2, all_test))` rmse.

Finally, for the linear regression models, we have to mathematically calculate rmse step by step since we use lrating instead of mrating in the models.

```{r adding predictions to linear regression models, include=FALSE}
all_test <- all_test %>% 
  add_predictions(mod_rating_budget, "predlrating_budget") %>%
  mutate(predrating_budget = 2^predlrating_budget) %>%
  add_predictions(mod_rating_runtime, "predlrating_runtime") %>%
  mutate(predrating_runtime = 2^predlrating_runtime) %>%
  add_predictions(mod_rating_releaseYear, "predlrating_releaseYear") %>%
  mutate(predrating_releaseYear = 2^predlrating_releaseYear) %>%
  add_predictions(mod_rating_genre, "predlrating_genre") %>%
  mutate(predrating_genre = 2^predlrating_genre) %>%
  add_predictions(mod_rating_all, "predlrating_all") %>%
  mutate(predrating_all = 2^predlrating_all)
```

The rmse for each model are `r comma(sqrt(mean((all_test$mrating - all_test$predrating_budget)^2)))`, `r comma(sqrt(mean((all_test$mrating - all_test$predrating_runtime)^2)))`, `r comma(sqrt(mean((all_test$mrating - all_test$predrating_releaseYear)^2)))`, `r comma(sqrt(mean((all_test$mrating - all_test$predrating_genre)^2)))`, and `r comma(sqrt(mean((all_test$mrating - all_test$predrating_all)^2)))` respectively. Clearly, the rmse of linear regression model with all the important variables are the lowest among all the linear regression models.

Now we continue to plot the residuals on the test data with the random forest model 2, the tree model 2, and the linear regression model with all the important variables, because these models have a relatively low rmse in each model category.

```{r adding predictions and residuals to test data, include=FALSE}
all_test <- all_test %>% 
  add_predictions(forest_rating2, "predrating_forest") %>%
  add_residuals(forest_rating2, "residrating_forest") %>%
  add_predictions(tree_rating2, "predrating_tree") %>%
  add_residuals(tree_rating2, "residrating_tree") %>%
  add_residuals(mod_rating_all, "residlrating_all") %>%
  mutate(residrating_all = 2^residlrating_all)
```

```{r residual histograms on test data,echo=FALSE}
plot_forest <- ggplot(all_test, aes(x = residrating_forest)) + geom_histogram(bins=30)
plot_tree <- ggplot(all_test, aes(x = residrating_tree)) + geom_histogram(bins=30)
plot_all <- ggplot(all_test, aes(x = residrating_all)) + geom_histogram(bins=30)

Plot_grid <- plot_grid(plot_forest, plot_tree, plot_all, ncol = 3, nrow = 1)
title <- ggdraw() + draw_label("Residual Distribution", fontface = 'bold')
plot_grid(title, Plot_grid, ncol = 1, rel_heights = c(0.2, 1.2))
```

From the residual distribution, we can conclude that random forest model and tree_based model both have an approximate mean of 0. However, the mean of the linear regression model roughly equals to 1, which is not good. This verifies the frst one of our hypothesis that the linear regression model is not a good model given this dataset. Compared with the tree_based model, the random forest model performs better with the narrower standard deviation. 

Next we proceed the predictions and residuals plot on the test data for the three models.

```{r prediction and residual plots on test data,echo=FALSE}
plot_forest_ <- 
  ggplot(all_test, aes(x = predrating_forest, y = residrating_forest)) +
  xlab("Fitted Movie Rating") + ylab("Residuals of Random Forest Model") +
  geom_point()
plot_tree_ <- 
  ggplot(all_test, aes(x = predrating_tree, y = residrating_tree)) +
  xlab("Fitted Movie Rating") + ylab("Residuals of Tree_based Model") +
  geom_point()
plot_all_ <- ggplot(all_test, aes(x = predrating_all, y = residrating_all)) +
    xlab("Fitted Movie Rating") + ylab("Residuals of Linear Regression Model") +
    geom_point()

Plot_grid <- plot_grid(plot_forest_, plot_tree_, plot_all_, ncol = 3, nrow = 1)
title <- ggdraw() + draw_label("Residual VS Fitted Values", fontface = 'bold')
plot_grid(title, Plot_grid, ncol = 1, rel_heights = c(0.2, 1.2))
```

Apparently, the predictions and residuals plots show a similar result as before. The mean of residuals are about 0, 0, 1. However, these plots provide some additional information, which is as the ratings go up, the mean of the residuals go down, although the range of the residuals seem unchanged. That is to say, the models tend to overestimate the ratings when the ratings are low; while the models tend to underestimate the ratings when the ratings are high, which is also relatively reasonable in reality.

### Conclusion

After calculation and visualization, we an easily tell that the random forest model with the independent variables containing runtime, budget, releaseYear, and genre performs best with the lowest rmse `r comma(rmse(forest_rating2, all_test))`. 

```{r rmse table display, echo=FALSE}
knitr::kable(all_test %>% summarise(linear_model =sqrt(mean((mrating - predrating_all)^2))) %>% mutate(forest_model = rmse(forest_rating2, all_test)) %>% mutate(tree_model = rmse(tree_rating2, all_test)) %>% pivot_longer(c("linear_model","forest_model","tree_model"), names_to = "model", values_to = "rmse"))
```

From the importance table below, we can find runtime is the most important variable in predicting movie ratings, followed by Horror, Documentary and Drama in the genre. Variables like releaseYear and budget also play an important role to predict.

```{r importance table of the best model, echo=FALSE}

knitr::kable(head(importance(forest_rating2, type = 1)[order(-importance(forest_rating2, type=1)),]), col.names = '%IncMSE')

```

This makes sense as well in reality. However, the rmse for the best model is still relatively high given the movie ratings scale (0-5). Therefore, we may need to take other variables like title, overview, actors and so on into consideration in order to have a better performance of the model, which confirms our second hypothesis that our current variables are not adequate in predicting movie ratings effectively.

### Further Research

We also created scatter plots to display the relationship between ratings on the Y-axis to revenue and popularity on the X-axis. The results of the initial plot displayed a strong positive correlation between rating and revenue. As the ratings of movies increases, the revenue of it tends to also increase. However, the second plot displayed a weak positive correlation between ratings and popularity. This indicates that while both variables tend to go up in response to one another, there is a lower likelihood of there being a relationship with the two variables.

```{r scatter plots for ratings,echo=FALSE}

plot(tibble_all$revenue, tibble_all$m_rating,
     xlim=c(0,1500000000) , ylim=c(0,5), 
     pch=20, 
     cex=2, 
     col="#69b3a2",
     xlab="Revenue", ylab="Ratings",
     main="Rating/Revenue"
)

plot(tibble_all$popularity, tibble_all$m_rating,
     xlim=c(0,80) , ylim=c(0,5), 
     pch=20, 
     cex=2, 
     col="#69b3a2",
     xlab="Popularity", ylab="Ratings",
     main="Rating/Popularity"
)
```






